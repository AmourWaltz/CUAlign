2024-11-05 11:12:31,141 - root - INFO - No BitsAndBytes use.

2024-11-05 11:12:31,141 - root - INFO - Loading model and tokenizer from /workspace/model/Meta-Llama-3-8B ...

2024-11-05 11:12:31,552 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).

2024-11-05 11:12:56,604 - root - INFO - Loading data from ./data/nqopen/raw/validation.json ...

2024-11-05 11:12:56,607 - root - INFO - Arguments:
Model Arguments: ModelArguments(model_name='llama3', lora_use=False, lora_weights='./exp/{}/train', model_suffix='no_lora', bnb_use=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, model_max_length=2048, use_fast=False, padding_side='right', trust_remote_code=False)
Data Arguments: DataArguments(data_dir='./data/{}/raw', dataset='nqopen', data_file='validation', prompt_dir='./prompt/', continue_generate=False, sample_interval=1)
Inference Arguments: InferenceArguments(icl_type='icl', output_dir='./data/nqopen/prep/llama3_nqopen_no_lora_icl/llama3_nqopen_no_lora_icl', data_suffix='sample_2k', num_sampling=2, temperature=0.5, top_p=1.0, top_k=40, num_beams=1, max_gen_length=24, repetition_penalty=1.1)

2024-11-05 11:12:56,607 - root - INFO - The number of dataset: 1805

2024-11-05 11:12:56,607 - root - INFO - Start sampling ...

2024-11-05 18:50:26,910 - root - INFO - Total elapsed time: 7h 37m 30s

2024-11-05 18:50:26,910 - root - INFO - Sampling is done.

2024-11-05 18:50:27,205 - root - INFO - Save to ./data/nqopen/prep/llama3_nqopen_no_lora_icl/llama3_nqopen_no_lora_icl/llama3_nqopen_validation_sample_2k.json

